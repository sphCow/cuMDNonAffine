#include "nonaffine.h"

/******************************************************************************/
/******************************* KERNEL BEGIN *********************************/
/******************************************************************************/

//**************** PBC KERNELS ******************************************//
__device__ inline double d_pbcx(double xx) { return (xx - dev_l.x * rint(xx/dev_l.x) );}
__device__ inline double d_pbcy(double yy) { return (yy - dev_l.y * rint(yy/dev_l.y) );}

//**************** KERNEL TO CALCULATE X ******************************//
__global__
void kernel_calc_X(double4 *r, long *pid, long *nebz_pid, double4 *ref_r ,
                   long *pid2gtid, double4 *X ,long N, int num_nebz) {

  long gtid = blockDim.x*blockIdx.x + threadIdx.x;
	if(gtid >= N) return;

  long my_pid, my_nebz_pid, my_nebz_gtid;
  double4 my_x, my_nebz_r, my_r, my_ref_r;
  my_x = make_double4(0.0, 0.0, 0.0, 0.0);

  my_pid = pid[gtid];
  my_r = r[gtid];
  my_ref_r = ref_r[my_pid];

  for(int n=0; n<num_nebz; n++) {
    my_nebz_pid = nebz_pid[num_nebz*my_pid+n];
    my_nebz_gtid = pid2gtid[my_nebz_pid];
    my_nebz_r = r[my_nebz_gtid];

    my_x.x += d_pbcx(my_nebz_r.x - my_r.x) * d_pbcx(ref_r[my_nebz_pid].x - my_ref_r.x);
    my_x.y += d_pbcx(my_nebz_r.x - my_r.x) * d_pbcy(ref_r[my_nebz_pid].y - my_ref_r.y);
    my_x.z += d_pbcy(my_nebz_r.y - my_r.y) * d_pbcx(ref_r[my_nebz_pid].x - my_ref_r.x);
    my_x.w += d_pbcy(my_nebz_r.y - my_r.y) * d_pbcy(ref_r[my_nebz_pid].y - my_ref_r.y);
  }

  X[gtid] = my_x;
}

/************************ KERNEL TO CALCULATE EPSILON *************************/
__global__
void kernel_calc_epsilon(double4 *X_, double4 *Y_, long* pid, double4 *epsilon, long N) {
  long gtid = blockDim.x*blockIdx.x + threadIdx.x;
	if(gtid >= N) return;

  long my_pid = pid[gtid];
  double4 X = X_[gtid];
  double4 Y = Y_[my_pid];
  double4 my_epsilon;

  double inv_mod_Yi = 1.0/(Y.x * Y.w - Y.y * Y.z);

  my_epsilon.x = inv_mod_Yi*(X.x*Y.w - X.y*Y.z) - 1.0;
  my_epsilon.y = inv_mod_Yi*(X.y*Y.x - X.x*Y.y);
  my_epsilon.z = inv_mod_Yi*(X.z*Y.w - X.w*Y.z);
  my_epsilon.w = inv_mod_Yi*(X.w*Y.x - X.z*Y.y) - 1.0;

  epsilon[gtid] = my_epsilon;
  
  //debug 
  //if(my_pid == 469 || my_pid == 4698 || my_pid == 13988) {
  //	printf("dbg eps:: pid = %d \t eps=%lf %lf %lf %lf\n",my_pid,my_epsilon.x,my_epsilon.y,my_epsilon.z,my_epsilon.w);
  //}
}

/************************* KERNEL TO CALCULATE CHI ************************/
//		Vec2d dr(pbcx(prj.r[0] - pri.r[0]), pbcy(prj.r[1] - pri.r[1]));
//		Vec2d dR(pbcx(pRj.r[0] - pRi.r[0]), pbcy(pRj.r[1] - pRi.r[1]));
//		
//		double A = dr.x - (1.0 + eps[0])*dR.x - (      eps[1])*dR.y;
//		double B = dr.y - (      eps[2])*dR.x - (1.0 + eps[3])*dR.y;
//		
//		chi += pow(A,2) + pow(B, 2.0);
//		
/************************* KERNEL TO CALCULATE CHI ************************/

__global__
void kernel_calc_chi2(double4  *r, 
											long     *pid, 
											long     *nebz_pid, 
											double4  *ref_r ,
											long     *pid2gtid, 
											double4  *epsilon, 
											double   *dev_chi2, 
											long     N, 
											int      num_nebz, 
											bool     add_hchi_forces, 
											double4  *dev_f,
//											double4  *dev_f_hchi,
											double   h) {

  long gtid = blockDim.x*blockIdx.x + threadIdx.x;
	if(gtid >= N) return;

  long my_pid, my_nebz_pid, my_nebz_gtid;
  double4 my_nebz_r, my_r, my_ref_r;

	// my contribution to hchi forces
	double4 my_f = make_double4(0.0,0.0,0.0,0.0);

  double my_chi2 = 0.0;
  double4 my_epsilon = epsilon[gtid];
  my_pid = pid[gtid];
  my_r = r[gtid];
  my_ref_r = ref_r[my_pid];

  for(int n=0; n<num_nebz; n++) {
    my_nebz_pid = nebz_pid[num_nebz*my_pid+n];
    my_nebz_gtid = pid2gtid[my_nebz_pid];
    my_nebz_r = r[my_nebz_gtid];

    double drx = d_pbcx(my_nebz_r.x - my_r.x);
    double dry = d_pbcy(my_nebz_r.y - my_r.y);

    double dRx = d_pbcx(ref_r[my_nebz_pid].x - my_ref_r.x);
    double dRy = d_pbcy(ref_r[my_nebz_pid].y - my_ref_r.y);

    //double chi = drx - (1.0 + my_epsilon.x)*dRx - my_epsilon.y*dRy + dry - my_epsilon.z*dRx - (my_epsilon.w+1.0)*dRy;
    
    double chi_x = drx - (1.0 + my_epsilon.x)*dRx - (my_epsilon.y      )*dRy;
    double chi_y = dry - (      my_epsilon.z)*dRx - (my_epsilon.w + 1.0)*dRy;
    
    my_chi2 += chi_x*chi_x + chi_y*chi_y;
    
    if(add_hchi_forces) {
  		my_f.x += chi_x;
  		my_f.y += chi_y;
  	}
  
  }

  dev_chi2[gtid] = my_chi2;
  
  if(add_hchi_forces) {
  		dev_f[gtid].x += -2.0*h*my_f.x;
  		dev_f[gtid].y += -2.0*h*my_f.y;
  		
  		//debug
  		//dev_f_hchi[gtid].x = -2.0*h*my_f.x;
  		//dev_f_hchi[gtid].y = -2.0*h*my_f.y;
  	}
}

/*
__global__
void kernel_calc_force_h_chi(double4 *r, long *pid, long *nebz_pid, 
                        double4 *ref_r, long *pid2gtid, double4 *epsilon, 
                        double4 *dev_f, long N, int num_nebz, double h) {
	
	long gtid = blockDim.x*blockIdx.x + threadIdx.x;
	if(gtid >= N) return;
	
	long my_pid, my_nebz_pid, my_nebz_gtid;
  double4 my_nebz_r, my_r, my_ref_r, my_nebz_epsilon;

  double4 my_f = make_double4(0.0,0.0,0.0,0.0);
  my_pid = pid[gtid];
  my_r = r[gtid];
  my_ref_r = ref_r[my_pid];

  for(int n=0; n<num_nebz; n++) {
    my_nebz_pid = nebz_pid[num_nebz*my_pid+n];
    my_nebz_gtid = pid2gtid[my_nebz_pid];
    
    my_nebz_r = r[my_nebz_gtid];
		my_nebz_epsilon = epsilon[my_nebz_gtid]; //???

    double drx = -d_pbcx(my_nebz_r.x - my_r.x);
    double dry = -d_pbcy(my_nebz_r.y - my_r.y);

    double dRx = -d_pbcx(ref_r[my_nebz_pid].x - my_ref_r.x);
    double dRy = -d_pbcy(ref_r[my_nebz_pid].y - my_ref_r.y);	
     
    my_f.x += 2.0*drx - (1.0 + my_nebz_epsilon.x)*dRx - (      my_nebz_epsilon.y)*dRy;               
    my_f.y += 2.0*dry - (      my_nebz_epsilon.z)*dRx - (1.0 + my_nebz_epsilon.w)*dRy;
  
  	//debug
  	//if(my_nebz_pid == 469 || my_nebz_pid == 4698 || my_nebz_pid == 13988) {
  	//	printf("dbg neb_eps:: pid/gtid = %ld %ld \t eps=%lf %lf %lf %lf\n",my_nebz_pid,my_nebz_gtid ,my_nebz_epsilon.x,my_nebz_epsilon.y,my_nebz_epsilon.z,my_nebz_epsilon.w);
  	//}
  
  }
    
  my_f.x *= (-2.0*h);
  my_f.y *= (-2.0*h);
    
  dev_f[gtid] += my_f; 
                   
}
*/
/******************************************************************************/
/******************************** KERNEL END **********************************/
/******************************************************************************/

NonAffine::NonAffine(void) {
  use_device = GPU;
}

void NonAffine::set_N_read_ref(long N_) {

  this->N = N_;
  std::cout << "NonAffine: set N to " << N << std::endl;
  allocate();
  read_reference_from_file();
  populate_nebz();
  calc_Y();

  fileIO->register_pp_qty("chi", 1,chi2,1);

}

void NonAffine::set_box_dim(double _lx, double _ly) {
  this->lx = _lx;
  this->ly = _ly;
  //std::cout << "NonAffine: L set to " << lx << " " << ly << std::endl;

  if(use_device == GPU) {
    double3 l = make_double3(lx,ly,0.0);
    cudaMemcpyToSymbol(dev_l, &l, sizeof(l));
  }
}

/************* PRIVATE STUFFS *******************/

void NonAffine::allocate() {
  //allocate per atom quantities
  X = new double4[N];
  Y = new double4[N];
  epsilon = new double4[N];
  ref_r = new double4[N];
  chi2 = new double[N];
  ref_pid = new long[N];
  nebz_pid = new long[num_nebz*N];

	// debug forces due to hchi
	//f_hchi = new double4[N];

  //allocate for gpu
  if(use_device == GPU) {
    cudaMalloc((void **) &dev_X, N*sizeof(*dev_X));
    cudaMalloc((void **) &dev_Y, N*sizeof(*dev_Y));
    cudaMalloc((void **) &dev_epsilon, N*sizeof(*dev_epsilon));
    cudaMalloc((void **) &dev_ref_r, N*sizeof(*dev_ref_r));
    cudaMalloc((void **) &dev_nebz_pid, N*num_nebz*sizeof(*dev_nebz_pid));
    cudaMalloc((void **) &dev_chi2, N*sizeof(*dev_chi2));
  
  	//debug forces hchi
  	// cudaMalloc((void **) &dev_f_hchi, N*sizeof(double4));
  }

  fileIO->register_pp_qty("chi2", 1, chi2, 1);
}

NonAffine::~NonAffine() {
  delete[] X;
  delete[] Y;
  delete[] epsilon;
  delete[] ref_r;
  delete[] chi2;
  delete[] ref_pid;
  delete[] nebz_pid;
}

//read 1st config
int NonAffine::read_reference_from_file() {
  std::ifstream f;
  f.open(ref_file_name.c_str() );

  //if(!f.good()) return -1;

  std::string line;
  int index = 0;
  //long lcount = 0; 
  //long particle_count = 0;
  
  long line_num = 0;
  long pid_read = -1;
  long header_line;
  double discard;

	std::cout << "--------------- NonAffine: Reading reference lattice ---------------" << std::endl;

  while(getline(f,line)) {
    /*
    std::stringstream iss(line);

    if(line[0]=='#' || line.empty()) continue; //skip empty lines && lines starting with #
    if(particle_count==N) break;
    
	//std::cout << line << std::endl;
    iss >> ref_pid[particle_count]
    		>> ref_r[particle_count].x
    		>> ref_r[particle_count].y;

    particle_count++;

    lcount++;
  */
  
  	if(line.find("NEW_FRAME") != std::string::npos) {
				index++;
			
				//if(index>=begin_index && index<=end_index) {
				if(index == 1) {	
					header_line = line_num; 
					std::cout << "current index -> " << index << "\n";
					//cout.flush();
				}
			
				else break;
			}
					
			//parse header values	
			if(line_num == header_line+1) {
				std::stringstream iss1(line);
				double htstep, hl[2], hN;
				iss1 >> htstep >> hl[0] >> hl[1] >> hN;
				pid_read = 0;
		
				//particles.resize(h.N);
				std::cout << "Header contents -> " << std::endl;
				std::cout << htstep << " " << hl[0] << " " << hl[1] << " " << hN << std::endl;
		
			//compare
		
			}
		
			//accumulate particle data
			if(line_num >= header_line+3 and pid_read<=N) {			
				std::stringstream iss(line);
			
				iss >> ref_pid[pid_read] 
						>> ref_r[pid_read].x 
						>> ref_r[pid_read].y;
		
				pid_read++;
			}
		
			line_num++;
		}
		
		std::cout << "NonAffine: Done reading reference lattice " << std::endl;
		for(int i=0; i<10; i++) std::cout << ref_pid[i] << "\t" << ref_r[i].x << "\t" << ref_r[i].x << std::endl;
		std::cout << "..." << std::endl;
		for(int i=N-10; i<N; i++) std::cout << ref_pid[i] << "\t" << ref_r[i].x << "\t" << ref_r[i].x << std::endl; 
		std::cout << "------------------------------------------------------------------------" << std::endl;
		
  //GPU - copy reference lattice to GPU
  if(use_device == GPU) {
    cudaMemcpy(dev_ref_r, ref_r, N*sizeof(*dev_ref_r), cudaMemcpyHostToDevice);
  }

  return 0;
}

void NonAffine::interpret(std::vector<std::string> tokenized) {  	
	std::vector<std::string>::iterator it;
	std::cout << std::endl << "|------------- Non-Affine stuffs -------------|" << std::endl;	
	
	// READ REFERENCE
  it = find(tokenized.begin(), tokenized.end(), "read_reference");
  if (it == tokenized.end()) {
    std::cout << "A reference lattice must be specified" << std::endl;
  } else {
    int pos = std::distance(tokenized.begin(), it);
    ref_file_name = tokenized[pos+1];
    std::cout << "reference lattice to be read from " << ref_file_name << std::endl;
  }
	
	// H_chi force
  it = find(tokenized.begin(), tokenized.end(), "h_chi");
	if (it == tokenized.end()) {
    std::cout << "Forces due to h_chi will NOT be added" << std::endl;
  	calc_force_hchi_required = false;
  } else {
    int pos = std::distance(tokenized.begin(), it);
    h = boost::lexical_cast<double>(tokenized[pos+1]);
    std::cout << "h =  " << h << std::endl;
    calc_force_hchi_required = true;
  }
  
  std::cout << "|---------------------------------------------|" << std::endl;

}

//populate neghibours
// ASSUME TRIANGULAR LATTICE WITH Nx=Ny=sqrt(N)
void NonAffine::populate_nebz() {

  // WARNING:: AS OF NOW IT ONLY WORKS FOR Nx=Ny
  long Nx = static_cast<long>(std::sqrt(N));
  long Ny = Nx;
  std::cout << "NonAffine: Nx/y " << Nx << " " << Ny << std::endl;

  for(long ix=0; ix<Nx; ix++) {
    for(long iy=0; iy<Ny; iy++) {
      long i=ix+iy*Nx;
      if(iy%2 == 0) {
        nebz_pid[num_nebz*i+0] = ((ix + Nx - 1) % Nx + ((iy + Ny + 1) % Ny) * Nx); //NW
        nebz_pid[num_nebz*i+1] = ((ix + Nx - 1) % Nx + ((iy + Ny - 1) % Ny) * Nx); //SW
      }
      else {
        nebz_pid[num_nebz*i+0] = ((ix + 1) % Nx + ((iy + Ny + 1) % Ny) * Nx); //NE
        nebz_pid[num_nebz*i+1] = ((ix + 1) % Nx + ((iy + Ny - 1) % Ny) * Nx); //SE
      }

      nebz_pid[num_nebz*i+2] = ((ix + 1) % Nx + ((iy + Ny) % Ny) * Nx); //EAST
      nebz_pid[num_nebz*i+3] = ((ix + Nx) % Nx + ((iy + Ny + 1) % Ny) * Nx); //N
      nebz_pid[num_nebz*i+4] = ((ix + Nx - 1) % Nx + ((iy + Ny) % Ny) * Nx); //W
      nebz_pid[num_nebz*i+5] = ((ix + Nx) % Nx + ((iy + Ny - 1) % Ny) * Nx); //S
    }
  }

  //GPU - Copy nebz to GPU
  if(use_device == GPU) {
    cudaMemcpy(dev_nebz_pid, nebz_pid, N*num_nebz*sizeof(*dev_nebz_pid), cudaMemcpyHostToDevice);
  }

  /*//DEBUG
  std::ofstream df("na.ref");
    for(int i=0; i<N; i++) {
      df << ref_pid[i] << " " << ref_r[i].x << " " << ref_r[i].y << "\t\t";
      for(int j=0; j<num_nebz; j++) df << nebz_pid[num_nebz*i+j] << " ";
      df << "\t\t";
      for(int j=0; j<num_nebz; j++) df << ref_r[nebz_pid[num_nebz*i+j]].x << " " << ref_r[nebz_pid[num_nebz*i+j]].y << "\t";
      df << std::endl;
    }
  df.close();
  */
}

void NonAffine::calc_Y() {
  for(long i=0; i<N; i++) {
    Y[i].x = 0.0;
    Y[i].y = 0.0;
    Y[i].z = 0.0;
    Y[i].w = 0.0;

    for(int n=0; n<num_nebz; n++) {
      Y[i].x += pbcx(ref_r[nebz_pid[num_nebz*i+n]].x - ref_r[i].x) * pbcx(ref_r[nebz_pid[num_nebz*i+n]].x - ref_r[i].x);
      Y[i].y += pbcx(ref_r[nebz_pid[num_nebz*i+n]].x - ref_r[i].x) * pbcy(ref_r[nebz_pid[num_nebz*i+n]].y - ref_r[i].y);
      Y[i].z += pbcy(ref_r[nebz_pid[num_nebz*i+n]].y - ref_r[i].y) * pbcx(ref_r[nebz_pid[num_nebz*i+n]].x - ref_r[i].x);
      Y[i].w += pbcy(ref_r[nebz_pid[num_nebz*i+n]].y - ref_r[i].y) * pbcy(ref_r[nebz_pid[num_nebz*i+n]].y - ref_r[i].y);
    }
  }
    
  if(use_device == GPU) {
    cudaMemcpy(dev_Y, Y, N*sizeof(*dev_Y), cudaMemcpyHostToDevice);
  }
  
  /*// print Y
  std::ofstream fy("Y.txt");
  fy << "# i X.x    X.y    X.z    X.w" << std::endl;
  for(long i=0; i<N; i++) {
  	fy << i << " " << Y[i].x << " " << Y[i].y << " " << Y[i].z << " " << Y[i].w << std::endl;
  }
  fy.close(); */

}

void NonAffine::calc_X(double4 *r) {

  if(use_device == CPU) {
    for(long i=0; i<N; i++) {
      //for(int j=0; j<4; j++) {
        X[i].x = 0.0;
        X[i].y = 0.0;
        X[i].z = 0.0;
        X[i].w = 0.0;
      //}

      for(int n=0; n<num_nebz; n++) {
        X[i].x += pbcx(r[nebz_pid[num_nebz*i+n]].x-r[i].x) * pbcx(ref_r[nebz_pid[num_nebz*i+n]].x-ref_r[i].x);
        X[i].y += pbcx(r[nebz_pid[num_nebz*i+n]].x-r[i].x) * pbcy(ref_r[nebz_pid[num_nebz*i+n]].y-ref_r[i].y);
        X[i].z += pbcy(r[nebz_pid[num_nebz*i+n]].y-r[i].y) * pbcx(ref_r[nebz_pid[num_nebz*i+n]].x-ref_r[i].x);
        X[i].w += pbcy(r[nebz_pid[num_nebz*i+n]].y-r[i].y) * pbcy(ref_r[nebz_pid[num_nebz*i+n]].y-ref_r[i].y);
      }
    }

  }

  // in GPU
  if(use_device == GPU) {
    kernel_calc_X<<<nblocks, nthreads>>>(dev_r, dev_pid, dev_nebz_pid, dev_ref_r , dev_pid2gtid, dev_X, N,num_nebz);
  }

}

void NonAffine::calc_epsilon() {
  if(use_device == CPU) {
    for(long i=0; i<N; i++) {
      // Y inverse
      double a,b,c,d,inv_mod_Yi;
      double p,q,r,s;
      a = Y[i].x;
      b = Y[i].y;
      c = Y[i].z;
      d = Y[i].w;
      inv_mod_Yi = 1.0/(a*d-b*c);

      p = X[i].x;
      q = X[i].y;
      r = X[i].z;
      s = X[i].w;

      epsilon[i].x = inv_mod_Yi*(p*d-q*c)-1.0;
      epsilon[i].y = inv_mod_Yi*(q*a-p*b);
      epsilon[i].z = inv_mod_Yi*(r*d-s*c);
      epsilon[i].w = inv_mod_Yi*(s*a-r*b)-1.0;
    }

    //mean epsilon
    mean_epsilon.x = 0.0;
    mean_epsilon.y = 0.0;
    mean_epsilon.z = 0.0;
    mean_epsilon.w = 0.0;

    for(long i=0; i<N; i++) {
      mean_epsilon.x += epsilon[i].x;
      mean_epsilon.y += epsilon[i].y;
      mean_epsilon.z += epsilon[i].z;
      mean_epsilon.w += epsilon[i].w;
    }

    mean_epsilon.x /= double(N);
    mean_epsilon.y /= double(N);;
    mean_epsilon.z /= double(N);;
    mean_epsilon.w /= double(N);;
  }

  //GPU
  if(use_device == GPU) {
    kernel_calc_epsilon<<<nblocks,nthreads>>>(dev_X, dev_Y, dev_pid, dev_epsilon, N);  
  }
  
  

}


void NonAffine::calc_chi2(double4 *r) { // r has to be sorted by pid

	//************* TODO HERE ************************ //
	map_pid2gtid(dev_pid,dev_pid2gtid,N,nblocks,nthreads);
	//*************************************************

  calc_X(r);
  calc_epsilon();

  if(use_device == CPU) {

    for(long i=0; i<N; i++) {
      chi2[i] = 0.0;

      for(int n=0; n<num_nebz; n++) {
        long npid = nebz_pid[num_nebz*i+n];

        double drx = pbcx(r[npid].x - r[i].x);
        double dry = pbcy(r[npid].y - r[i].y);
        double dRx = pbcx(ref_r[npid].x - ref_r[i].x);
        double dRy = pbcy(ref_r[npid].y - ref_r[i].y);

        double e11 = epsilon[i].x + 1.0;
        double e12 = epsilon[i].y;
        double e21 = epsilon[i].z;
        double e22 = epsilon[i].w + 1.0;

        double chi = drx - e11*dRx - e12*dRy + dry - e21*dRx - e22*dRy;
        chi2[i] += chi*chi;
      }

    }

    //mean
    mean_chi2 = 0.0;
    for(long i=0; i<N; i++) mean_chi2 += chi2[i];
    mean_chi2/=double(N);
  }

  if(use_device == GPU) {
    //gpu
    /*
    void kernel_calc_chi2(double4  *r, 
											long     *pid, 
											long     *nebz_pid, 
											double4  *ref_r ,
											long     *pid2gtid, 
											double4  *epsilon, 
											double   *dev_chi2, 
											long     N, 
											int      num_nebz, 
											bool     add_hchi_forces, 
											double4  *dev_f,
											double   h) {
    */
    
    kernel_calc_chi2<<<nblocks,nthreads>>>( dev_r, 
    																				dev_pid, 
    																				dev_nebz_pid, 
    																				dev_ref_r ,
    																				dev_pid2gtid, 
    																				dev_epsilon, 
    																				dev_chi2, 
    																				N, 
    																				num_nebz,
    																				calc_force_hchi_required,
    																				dev_f,
    																				h);

    thrust::device_ptr<double> dev_ptr_chi2 = thrust::device_pointer_cast(dev_chi2);
    mean_chi2 = thrust::reduce(dev_ptr_chi2, dev_ptr_chi2 + N)/static_cast<double>(N);
    
  }

}

void NonAffine::get_pp_chi2() {
  if(use_device == GPU) {
    cudaMemcpy(host_pid, dev_pid, N*sizeof(*host_pid), cudaMemcpyDeviceToHost);
    cudaMemcpy(chi2, dev_chi2, N*sizeof(double), cudaMemcpyDeviceToHost);
    thrust::sort_by_key(host_pid, host_pid+N, chi2);
  
  	cudaMemcpy(X, dev_X, N*sizeof(double), cudaMemcpyDeviceToHost);
    thrust::sort_by_key(host_pid, host_pid+N, X);
    
  	//debug
  	//cudaMemcpy(f_hchi, dev_f_hchi, N*sizeof(double4), cudaMemcpyDeviceToHost);
  	//thrust::sort_by_key(host_pid, host_pid+N, f_hchi);
  	//std::ofstream fdf("dbg_fhchi.txt", std::ios::out | std::ios::app);
  	//
  	//for(int i=0; i<N; i++) {
  	//	fdf << host_pid[i] << "\t" << chi2[i] << "\t" << f_hchi[i].x << "\t" << f_hchi[i].y << "\t" << std::endl;
  	//}
  	//
  	//fdf << std::endl << std::endl;
  
  }

}


double NonAffine::pbcx(double xx) {
  return (xx - lx * rint(xx/lx) );
}

double NonAffine::pbcy(double yy) {
  return (yy - ly * rint(yy/ly) );
}

//
void NonAffine::scale_ref_lattice(double sx,double sy) {
  for(long i=0; i<N; i++) {
    ref_r[i].x = (1.0 + sx)*ref_r[i].x;
  	ref_r[i].y = (1.0 + sy)*ref_r[i].y;
  }

  //GPU - copy reference lattice to GPU
  if(use_device == GPU) {
    cudaMemcpy(dev_ref_r, ref_r, N*sizeof(*dev_ref_r), cudaMemcpyHostToDevice);
  }

  calc_Y();
  //std::cout << "NonAffine: reference lattice " << std::endl;


}
